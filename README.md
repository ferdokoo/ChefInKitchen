# About
<img src="./media/ChefInKitchen_thumbnail.png?raw=true" width="50%">
This is my first experiment with Google Antigravity to code a client-server app without prior knowledge of React. 

Inspired by a <a href="https://ai-partner-catalyst.devpost.com/?_gl=1*1u1pxfx*_ga_0YHJK3Y10M*czE3NjUyNzk1MDMkbzEkZzEkdDE3NjUyNzk1NjQkajYwJGwwJGgw">challenge on DevPost</a>

## Slogan
Cooking apps require screens; hands get dirty; instructions are rigid. I propose a real-time conversational cooking companion with ElevenLabs for natural live voice and Gemini for meal reasoning.

## High-level architecture overview
The app consists of **three layers**:

1. **Client (React + ElevenLabs SDK)** — captures voice, streams audio, plays the chef’s voice, and maintains UI.
2. **Conversation Orchestrator (ElevenLabs Agent)** — manages turn-taking, emotion analysis, voice personas.
3. **Reasoning + Recipe Engine (Google Cloud Gemini / Vertex AI)** — handles recipe creation, substitutions, step transitions, timers, and adaptive logic.

<img src="./media/High%20level%20diagram%201.jpeg?raw=true" width="50%">


# Repository structure
1. **chef-kitchen.md** - project description that was generated by chatGPT and used by Antogravity as a specification for the project
2. **google_ai_setup.md** - guide on how to get API key for Gemini
3. **agent_tools** - folder with guide and JSON configuration for Elevenlabs agent tools
4. **client** - client code on React + Vite
5. **server** - server code with deploy_cloud_run.md guide
6. **media** - supporting images generated in chatGPT and MS Copilot
 
